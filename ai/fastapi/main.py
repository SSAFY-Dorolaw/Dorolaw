from fastapi import FastAPI, File, UploadFile, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import shutil
import os
import sys
import threading
import json
import requests
import time
import pika

RABBITMQ_HOST = 'rabbitmq'
QUEUES_CONFIG = {
    'main_diagnosis_queue': {
        'result_endpoint': 'http://backend:8080/api/alarms/analysis'
    },
    'requests_queue': {
        'result_endpoint': 'http://backend:8080/api/alarms/requests'
    }
}

# Importing recognizer modules
sys.path.append('/dorolaw/ai-model/')
from recognizer.single_tsn_recognizer import Single_tsn_recognizer
from recognizer.yolo_tsn_recognizer import Yolo_tsn_recognizer

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = FastAPI(title="main", description="AI recognition for fault analysis", version="1.0")

# Recognizer ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
rcn = Single_tsn_recognizer()
# rcn = Yolo_tsn_recognizer()

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì • (ì˜ˆ: CSS íŒŒì¼)
app.mount("/ai/static", StaticFiles(directory="static"), name="static")

def map_result_to_payload(request_id, result, member_id, queue_name, data):
    """AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œ ìš”ì²­ í˜•ì‹ì— ë§ê²Œ ë§¤í•‘"""
    # ê³µí†µ í˜•ì‹ì˜ í˜ì´ë¡œë“œ ìƒì„±
    payload = {
        "memberId": member_id,
        "accidentObject": result.get('AccidentObject', 'ì°¨ëŸ‰'),
        "accidentLocation": result.get('AccidentLocation', 'ë„ë¡œ'),
        "accidentLocationCharacteristics": result.get('AccidentLocationCharacteristics', 'ì¼ë°˜ë„ë¡œ'),
        "directionOfA": result.get('DirectionOfA', 'ì§ì§„'),
        "directionOfB": result.get('DirectionOfB', 'ì§ì§„'),
        "faultRatioA": result.get('FaultRatioA', 50),
        "faultRatioB": result.get('FaultRatioB', 50),
        "accidentType": result.get('AccidentType', 1)
    }
    
    # íë³„ë¡œ ë‹¤ë¥¸ content ì„¤ì •
    if queue_name == 'main_diagnosis_queue':
        payload["faultAnalysisId"] = data.get('faultAnalysisId')
        payload["content"] = f"êµí†µ ì‚¬ê³  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ID: {payload['faultAnalysisId']}"
    elif queue_name == 'requests_queue':
        payload["requestId"] = request_id
        payload["content"] = f"ìƒë‹´ ìš”ì²­ì— ëŒ€í•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ID: {request_id}"
    
    return payload

def send_result_to_backend(payload, endpoint):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¡œ ì „ì†¡"""
    try:
        print(payload)
        response = requests.post(
            endpoint,
            json=payload,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            print(f"âœ… ê²°ê³¼ ì „ì†¡ ì„±ê³µ: {endpoint}, requestId={payload['requestId']}")
            return True
        else:
            print(f"âš ï¸ ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {endpoint}, status={response.status_code}, response={response.text}")
            return False
    except Exception as e:
        print(f"ğŸš¨ ê²°ê³¼ ì „ì†¡ ì˜¤ë¥˜: {endpoint}, error={e}")
        return False

def process_message(body, queue_name):
    """ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§ ì‘ì„±"""
    try:
        # ë¨¼ì € ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ UTF-8 ë¬¸ìì—´ë¡œ ë³€í™˜
        message_str = body.decode('utf-8')
        data = json.loads(message_str)
        print("ğŸ”” ì§„ë‹¨ ìš”ì²­ ì²˜ë¦¬:", data)

        # ì—¬ê¸° ì²˜ë¦¬ ë¡œì§ ë“¤ì–´ê°€ë©´ ë¨
        # JSONì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        request_id = data.get('requestId')
        file_name = data.get('fileName')
        member_id = data.get('memberId')
        
        if not file_name:
            print("âŒ íŒŒì¼ ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
            
        # íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        video_path = f"/home/ubuntu/videos/{file_name}"
        
        if not os.path.exists(video_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return
            
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path}")
        
        # AI ë¶„ì„ ì‹¤í–‰
        result = rcn.predict(video_path)
        
        print(f"âœ… ë¶„ì„ ê²°ê³¼: {result}")

        
        # ê²°ê³¼ë¥¼ ë°±ì—”ë“œ API ìš”ì²­ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        payload = map_result_to_payload(request_id, result, member_id, queue_name, data)
        
        # ê²°ê³¼ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡
        endpoint = QUEUES_CONFIG[queue_name]['result_endpoint']
        send_result_to_backend(payload, endpoint)
        
        # ì—¬ê¸°ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¡œ ì „ì†¡í•˜ëŠ” ë¡œì§ ì¶”ê°€
        # ì˜ˆ: publish_result(request_id, result)

    except Exception as e:
        print("ğŸš¨ ì²˜ë¦¬ ì˜¤ë¥˜:", e)

def start_queue_consumer(queue_name):
    """RabbitMQ ì†Œë¹„ì í•¨ìˆ˜"""
    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    max_retries = 30
    retry_interval = 2  # ì´ˆ
    
    for attempt in range(max_retries):
        try:
            connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host=RABBITMQ_HOST, 
                    credentials=pika.PlainCredentials('guest', 'guest'),
                    connection_attempts=3,
                    retry_delay=2
                )
            )
            channel = connection.channel()
            channel.queue_declare(queue=queue_name, durable=True)
            
            def callback(ch, method, properties, body):
                process_message(body, queue_name)
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
            channel.basic_qos(prefetch_count=1)
            channel.basic_consume(queue=queue_name, on_message_callback=callback)
            print("â–¶ï¸ RabbitMQ ì†Œë¹„ì ì‹œì‘ ì„±ê³µ...")
            channel.start_consuming()
            break  # ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ë©´ ë£¨í”„ ì¢…ë£Œ
            
        except Exception as e:
            print(f"RabbitMQ ì—°ê²° ì‹œë„ {attempt+1}/{max_retries} ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                print(f"{retry_interval}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(retry_interval)
            else:
                print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. RabbitMQ ì—°ê²° ì‹¤íŒ¨.")

@app.on_event("startup")
def startup_event():
    # FastAPI ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  í ì†Œë¹„ì ìŠ¤ë ˆë“œ ì‹œì‘
    print("â–¶ï¸ FastAPI ì„œë²„ ì‹œì‘...")
    
    # ê° íë§ˆë‹¤ ë³„ë„ì˜ ì†Œë¹„ì ìŠ¤ë ˆë“œ ì‹œì‘
    for queue_name in QUEUES_CONFIG.keys():
        consumer_thread = threading.Thread(
            target=start_queue_consumer,
            args=(queue_name,),
            daemon=True
        )
        consumer_thread.start()
        print(f"â–¶ï¸ {queue_name} ì†Œë¹„ì ìŠ¤ë ˆë“œ ì‹œì‘ë¨")

# ê¸°ë³¸ ê²½ë¡œ ì •ì˜
@app.get("/ai")
async def root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ë¹„ë””ì˜¤ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
@app.post("/ai/predict")
async def predict(video: UploadFile = File(...)):
    save_path = f"./video_save/{video.filename}"
    try:
        # ë¹„ë””ì˜¤ íŒŒì¼ì„ ì €ì¥
        with open(save_path, "wb") as buffer:
            shutil.copyfileobj(video.file, buffer)

        # ë¹„ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ
        result = rcn.predict(save_path)

        # ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return JSONResponse(content=result)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        # íŒŒì¼ ì‚­ì œ
        if os.path.exists(save_path):
            os.remove(save_path)

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)
